# resume_parser.py
import os
import fitz           # PyMuPDF
import easyocr
import spacy
from spacy.matcher import Matcher
import re
from typing import List
from dateparser.search import search_dates

# 1. OCR reader (module‚Äêlevel so it loads once)
reader = easyocr.Reader(["en"])

# 2. spaCy model + matcher (load once at import)
nlp = spacy.load("en_core_web_sm")
matcher = Matcher(nlp.vocab)
SECTIONS = ["education", "experience", "skills",
            "projects", "certifications", "publications"]

# Build and register patterns (sections)
for sec in SECTIONS:
    # Use uppercase labels but match on token.lower_ for case-insensitive section headings detection.
    matcher.add(sec.upper(), [[{"LOWER": sec}]])


def find_headings(text: str) -> list[tuple[str, int]]:
    """Return a sorted list of (SECTION_NAME, token_start_index)."""
    doc = nlp(text) # converts text into a data structure usable by spacy
    matches = matcher(doc)  # creates a list of tuples that each have a section header, and start/end points as integers
     # 'mid' is the integer id that was assigned to the section header, we use that to get the header name
     # 'start' is the start index for that header section in 'matches' (above) '_' is the end point (not used)
    headings = [(nlp.vocab.strings[mid], start) for mid, start, _ in matches]

     # sort headings data by 2nd item ('start' index)
    headings = sorted(headings, key=lambda x: x[1])

    return headings


def split_into_sections(text: str) -> dict[str, str]:
    """Build a dictionary by mapping section headings to the section's body text."""
    doc = nlp(text)
    heads = find_headings(text)
    heads.append(("END", len(doc)))  # sentinel

    sections = {}
    for (sec, start), (_, nxt) in zip(heads, heads[1:]):
        span = doc[start:nxt].text.strip().splitlines()
        # remove heading itself
        content = "\n".join(span[1:]).strip()
        sections[sec] = content

    return sections

def getExt(file_name):
    """Returns extension of file_name as string"""
    name, extension = os.path.splitext(file_name)
    return extension


def parseFileToText(fileName):
    """Parses file at fileName and returns it as a string"""
    extension = getExt(fileName)
    result_text = ""

    if(extension == ".pdf"):
        doc = fitz.open(fileName)  # Load the PDF

        # Extract text from each page
        for page in doc:
            result_text += page.get_text()  
    elif (extension == ".txt"):
        # Parse to string
        result_text = open("resumes/resumeBbrim.txt", encoding="utf-8").read()
    elif(extension == ".jpg" or extension == ".png"):
        ocr_results = reader.readtext(fileName) # parse with OCR
        texts = [text for _, text, _ in ocr_results]    # Grab recognized strings
        result_text = " ".join(texts)   # Join parsed strings
    
    return result_text

def parseFileAtPathToText(filePath):
    """Parses file at fileName and returns it as a string"""
    extension = getExt(filePath)
    result_text = ""

    if(extension == ".pdf"):
        doc = fitz.open(filePath)  # Load the PDF

        # Extract text from each page
        for page in doc:
            result_text += page.get_text()  
    elif (extension == ".txt"):
        # Parse to string
        result_text = open(filePath, encoding="utf-8").read()
    elif(extension == ".jpg" or extension == ".png"):
        ocr_results = reader.readtext(filePath) # parse with OCR
        texts = [text for _, text, _ in ocr_results]    # Grab recognized strings
        result_text = " ".join(texts)   # Join parsed strings
    
    # print(f"Results text for resume parsing: {result_text}")

    return result_text

# ‚îÄ‚îÄ SMART FIELDS EXTRACTION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# degree‚Äêline indicators
DEGREE_KEYWORDS = [
    r"\bBachelor\b", r"\bB\.S\.?\b", r"\bBA\b", r"\bBSc\b",
    r"\bMaster\b",   r"\bM\.S\.?\b", r"\bMA\b",  r"\bMSc\b",
    r"\bPh\.?D\b",   r"\bDoctor\b",      r"\bAssociate\b"
]

def extract_name(text: str) -> str:
# 1) Look at the *very first* non‚Äêempty line and see if it looks like a human name
    for line in text.splitlines():
        candidate = line.strip()
        if not candidate:
            continue
        # simple heuristic: between 2‚Äì4 words, each capitalized
        parts = candidate.split()
        if 2 <= len(parts) <= 4 and all(p[0].isupper() for p in parts):
            print(f"Name parsed as (line‚Äê1 heuristic): {candidate}")
            return candidate
        break

    # 2) Regex ‚ÄúName: John Doe‚Äù anywhere
    m = re.search(r'(?mi)^Name[:\s]+(.+)$', text)
    if m:
        nm = m.group(1).strip()
        print(f"Name parsed via regex: {nm}")
        return nm

    # 3) Finally, fall back to spaCy NER in case the above failed
    doc = nlp(text)
    for ent in doc.ents:
        if ent.label_ == "PERSON":
            print(f"Name parsed via NER: {ent.text}")
            return ent.text

    return ""

def extract_location(text: str) -> str:
# 1) Look for a ‚Äúüìç City, State‚Äù pattern on the top line
    m = re.search(r'üìç\s*([^|]+)', text)
    if m:
        loc = m.group(1).strip()
        print(f"Location parsed via bullet: {loc}")
        return loc

    # 2) Regex capture ‚ÄúCity, ST‚Äù
    m2 = re.search(r'([A-Z][a-z]+(?: [A-Z][a-z]+)*,\s*[A-Z]{2})', text)
    if m2:
        loc = m2.group(1)
        print(f"Location parsed via regex: {loc}")
        return loc

    # 3) Fallback to spaCy GPE
    doc = nlp(text)
    for ent in doc.ents:
        if ent.label_ == "GPE":
            print(f"Location parsed via NER: {ent.text}")
            return ent.text

    return ""

def extract_email(text: str) -> str:
    """Find the first email address in the text."""
    m = re.search(r'[\w\.-]+@[\w\.-]+\.\w+', text)
    email = m.group(0) if m else ""
    if email:
        print(f"Email parsed as: {email}")
    else:
            print("extracting email failed.")
    return email

def extract_phone(text: str) -> str:
    """
    Find the first US‚Äêstyle phone number, e.g.
    (408) 555-1278, 408-555-1278, 408.555.1278, +1 408 555 1278
    """
    m = re.search(
        r'(\+?\d{1,2}\s*)?'                    # optional country code
        r'(\(\d{3}\)|\d{3})[\s\-.]?'          # area code
        r'\d{3}[\s\-.]?\d{4}',                # local number
        text
    )
    phone = m.group(0) if m else ""
    if phone:
        print(f"Phone parsed as: {phone}")
    else:
            print("extracting phone number failed.")
    return phone

def extract_degrees(text: str):
    """
    Scans each line for degree keywords.
    Returns two lists of tuples: (full line, date-string or ''):
      earned      ‚Äì true degrees
      in_progress ‚Äì lines with 'expected' or 'in progress'
    """
    earned, in_prog = [], []
    for line in text.splitlines():
        for pat in DEGREE_KEYWORDS:
            if re.search(pat, line, re.IGNORECASE):
                # pull any date-like substring
                result = search_dates(line)
                date_str = result[0][0] if result else ""

                target = in_prog if re.search(r"expected|in progress|ongoing",
                                               line, re.IGNORECASE) else earned
                target.append((line.strip(), date_str))
                break

    print(f"degrees parsed as {earned} .. {in_prog}")
    return earned, in_prog

def parse_resume(path: str) -> dict:
    """
    Master entrypoint:
      ‚Ä¢ parses raw text
      ‚Ä¢ extracts name, location
      ‚Ä¢ splits out earned vs in‚Äêprogress degrees
    """
    text = parseFileAtPathToText(path)
    name, location      = extract_name(text), extract_location(text)
    earned, in_prog     = extract_degrees(text)
    email               = extract_email(text)
    phone               = extract_phone(text)

    # ‚îÄ‚îÄ GPA extraction ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def extract_gpa(txt: str) -> float | None:
        # look for patterns like "GPA: 3.82" or "GPA 3.8/4.0"
        m = re.search(r"GPA[:\s]*([0-4](?:\.\d{1,2})?)", txt, re.IGNORECASE)
        return float(m.group(1)) if m else None

    gpa = extract_gpa(text)

    return {
        "text": text,
        "name": name,
        "location": location,
        "email":                email,
        "phone":                phone,
        "gpa": gpa,
        "degrees_earned": earned,
        "degrees_in_progress": in_prog
    }


import re
from typing import List

import re
from typing import List

import re
from typing import List

def split_projects_by_bullets(projects_text: str) -> List[str]:
    """
    Split a PROJECTS section into individual projects.
    1) Merge any wrapped‚Äêaround lines (starting with whitespace or lowercase)
       onto the previous line.
    2) Split into project blocks by finding bullets (‚ñ™ or ‚Ä¢) and slicing
       after each bullet‚Äêrun ends.
    3) Everything before the first bullet is kept as Project #1.
    """
    if not projects_text or not projects_text.strip():
        return []

    # 1) Normalize wraps: re‚Äêjoin lines that start with space or lowercase
    raw = projects_text.splitlines()
    normalized = []
    for line in raw:
        stripped = line.strip()
        if not stripped:
            continue
        # if this line is a wrap (starts with space or lowercase), merge it
        if normalized and (line.startswith(" ") or stripped[0].islower()):
            normalized[-1] += " " + stripped
        else:
            normalized.append(stripped)

    # 2) Find all bullet indices in normalized
    bullet_re = re.compile(r'^[‚ñ™‚Ä¢]')
    bullet_idxs = [i for i, L in enumerate(normalized) if bullet_re.match(L)]
    if not bullet_idxs:
        # no bullets ‚Üí one big project
        return ["\n".join(normalized).strip()]

    # 3) Group contiguous bullets into runs
    runs = []
    run = [bullet_idxs[0]]
    for idx in bullet_idxs[1:]:
        if idx == run[-1] + 1:
            run.append(idx)
        else:
            runs.append(run)
            run = [idx]
    runs.append(run)

    # 4) For each run, we split *after* its last bullet
    last_bullets = [r[-1] for r in runs]

    # 5) Build slice boundaries: start=0, then each last_bullet+1, then end
    starts = [0] + [i+1 for i in last_bullets] + [len(normalized)]

    # 6) Slice out each project
    projects = []
    for s, e in zip(starts, starts[1:]):
        block = normalized[s:e]
        text = "\n".join(block).strip()
        if text:
            projects.append(text)

    return projects




"""
def printProjects_from_sections(resume_path):
    print("printProjects_from_sections run.")

    myText     = parseFileToText(resume_path)
    mySections = split_into_sections(myText)
    raw_projects = mySections.get("PROJECTS", "")
    print(f"raw_projects is: {raw_projects}")
    project_list = split_projects_by_bullets(raw_projects)
    for i, proj in enumerate(project_list, 1):
        print(f"‚îÄ‚îÄ Project #{i} ‚îÄ‚îÄ")
        print(proj)
        print()
"""